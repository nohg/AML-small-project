{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression using neural network models\n",
    "\n",
    "Estimate (i.e. make regression for) the energy of electrons. This should be based on maximum 15 variables from the Electron Variable List. The target variable for this task is \"p_truth_E\": Energy (in GeV) of the electrons, and you should only train on real (i.e. truth identified, \"Truth==1\") electrons. Note: It is an advantage to ONLY train the regression on true electrons (Truth = 1), but when submitting the solution, the regression estimate should be applied to ALL candidates, as you don't (perfectly) know, which are electrons, and which are not. We evaluate algorithm performance by considering Mean Absolute Error (MAE) on relative estimate accuracy: (E_pred-E_true)/E_true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by opening the files and loading them into a Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    with h5py.File(f'{name}.h5', 'r') as f:\n",
    "        return pandas.DataFrame(f[name][:])\n",
    "\n",
    "train = load_data('train')\n",
    "test  = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## temporarily use a fraction of the data to speed everything up\n",
    "#train=train.sample(frac = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can verify the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data set: (162500, 166)\n",
      "Shape of test data set: (160651, 164)\n"
     ]
    }
   ],
   "source": [
    "print (f'Shape of training data set: {train.shape}')\n",
    "print (f'Shape of test data set: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the test set contains 2 columns less: `Truth` and `p_truth_E`.\n",
    "    \n",
    "Then we copy the variable list from the course website <https://www.nbi.dk/~petersen/Teaching/ML2020/SmallProject/VariableList.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables = ['actualInteractionsPerCrossing', 'averageInteractionsPerCrossing', 'correctedActualMu', 'correctedAverageMu', 'correctedScaledActualMu', 'correctedScaledAverageMu', 'NvtxReco', 'p_nTracks', 'p_pt_track', 'p_eta', 'p_phi', 'p_charge', 'p_qOverP', 'p_z0', 'p_d0', 'p_sigmad0', 'p_d0Sig', 'p_EptRatio', 'p_dPOverP', 'p_z0theta', 'p_etaCluster', 'p_phiCluster', 'p_eCluster', 'p_rawEtaCluster', 'p_rawPhiCluster', 'p_rawECluster', 'p_eClusterLr0', 'p_eClusterLr1', 'p_eClusterLr2', 'p_eClusterLr3', 'p_etaClusterLr1', 'p_etaClusterLr2', 'p_phiClusterLr2', 'p_eAccCluster', 'p_f0Cluster', 'p_etaCalo', 'p_phiCalo', 'p_eTileGap3Cluster', 'p_cellIndexCluster', 'p_phiModCalo', 'p_etaModCalo', 'p_dPhiTH3', 'p_R12', 'p_fTG3', 'p_weta2', 'p_Reta', 'p_Rphi', 'p_Eratio', 'p_f1', 'p_f3', 'p_Rhad', 'p_Rhad1', 'p_deltaEta1', 'p_deltaPhiRescaled2', 'p_TRTPID', 'p_TRTTrackOccupancy', 'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_numberOfSCTHits', 'p_numberOfTRTHits', 'p_numberOfTRTXenonHits', 'p_chi2', 'p_ndof', 'p_SharedMuonTrack', 'p_E7x7_Lr2', 'p_E7x7_Lr3', 'p_E_Lr0_HiG', 'p_E_Lr0_LowG', 'p_E_Lr0_MedG', 'p_E_Lr1_HiG', 'p_E_Lr1_LowG', 'p_E_Lr1_MedG', 'p_E_Lr2_HiG', 'p_E_Lr2_LowG', 'p_E_Lr2_MedG', 'p_E_Lr3_HiG', 'p_E_Lr3_LowG', 'p_E_Lr3_MedG', 'p_ambiguityType', 'p_asy1', 'p_author', 'p_barys1', 'p_core57cellsEnergyCorrection', 'p_deltaEta0', 'p_deltaEta2', 'p_deltaEta3', 'p_deltaPhi0', 'p_deltaPhi1', 'p_deltaPhi2', 'p_deltaPhi3', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhiRescaled0', 'p_deltaPhiRescaled1', 'p_deltaPhiRescaled3', 'p_e1152', 'p_e132', 'p_e235', 'p_e255', 'p_e2ts1', 'p_ecore', 'p_emins1', 'p_etconeCorrBitset', 'p_ethad', 'p_ethad1', 'p_f1core', 'p_f3core', 'p_maxEcell_energy', 'p_maxEcell_gain', 'p_maxEcell_time', 'p_maxEcell_x', 'p_maxEcell_y', 'p_maxEcell_z', 'p_nCells_Lr0_HiG', 'p_nCells_Lr0_LowG', 'p_nCells_Lr0_MedG', 'p_nCells_Lr1_HiG', 'p_nCells_Lr1_LowG', 'p_nCells_Lr1_MedG', 'p_nCells_Lr2_HiG', 'p_nCells_Lr2_LowG', 'p_nCells_Lr2_MedG', 'p_nCells_Lr3_HiG', 'p_nCells_Lr3_LowG', 'p_nCells_Lr3_MedG', 'p_pos', 'p_pos7', 'p_poscs1', 'p_poscs2', 'p_ptconeCorrBitset', 'p_ptconecoreTrackPtrCorrection', 'p_r33over37allcalo', 'p_topoetconeCorrBitset', 'p_topoetconecoreConeEnergyCorrection', 'p_topoetconecoreConeSCEnergyCorrection', 'p_weta1', 'p_widths1', 'p_widths2', 'p_wtots1', 'p_e233', 'p_e237', 'p_e277', 'p_e2tsts1', 'p_ehad1', 'p_emaxs1', 'p_fracs1', 'p_DeltaE', 'p_E3x5_Lr0', 'p_E3x5_Lr1', 'p_E3x5_Lr2', 'p_E3x5_Lr3', 'p_E5x7_Lr0', 'p_E5x7_Lr1', 'p_E5x7_Lr2', 'p_E5x7_Lr3', 'p_E7x11_Lr0', 'p_E7x11_Lr1', 'p_E7x11_Lr2', 'p_E7x11_Lr3', 'p_E7x7_Lr0', 'p_E7x7_Lr1' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we divide the training data into data (`X`) and targets (`y`)\n",
    "\n",
    "Only use the entries which we know are true electrons for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (121495, 160)\n",
      "Shape of y: (121495,)\n"
     ]
    }
   ],
   "source": [
    "train = train.loc[train['Truth']==1]\n",
    "X = train[all_variables]\n",
    "y = train['p_truth_E']\n",
    "\n",
    "print (f'Shape of X: {X.shape}')\n",
    "print (f'Shape of y: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualInteractionsPerCrossing</th>\n",
       "      <th>averageInteractionsPerCrossing</th>\n",
       "      <th>correctedActualMu</th>\n",
       "      <th>correctedAverageMu</th>\n",
       "      <th>correctedScaledActualMu</th>\n",
       "      <th>correctedScaledAverageMu</th>\n",
       "      <th>NvtxReco</th>\n",
       "      <th>p_nTracks</th>\n",
       "      <th>p_pt_track</th>\n",
       "      <th>p_eta</th>\n",
       "      <th>...</th>\n",
       "      <th>p_E5x7_Lr0</th>\n",
       "      <th>p_E5x7_Lr1</th>\n",
       "      <th>p_E5x7_Lr2</th>\n",
       "      <th>p_E5x7_Lr3</th>\n",
       "      <th>p_E7x11_Lr0</th>\n",
       "      <th>p_E7x11_Lr1</th>\n",
       "      <th>p_E7x11_Lr2</th>\n",
       "      <th>p_E7x11_Lr3</th>\n",
       "      <th>p_E7x7_Lr0</th>\n",
       "      <th>p_E7x7_Lr1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>42011.792969</td>\n",
       "      <td>1.834715</td>\n",
       "      <td>...</td>\n",
       "      <td>215.588013</td>\n",
       "      <td>43402.332031</td>\n",
       "      <td>74045.820312</td>\n",
       "      <td>337.980713</td>\n",
       "      <td>273.708801</td>\n",
       "      <td>43091.683594</td>\n",
       "      <td>74447.539062</td>\n",
       "      <td>470.177124</td>\n",
       "      <td>273.708801</td>\n",
       "      <td>43091.683594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>44975.042969</td>\n",
       "      <td>-2.023659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27372.955078</td>\n",
       "      <td>104002.000000</td>\n",
       "      <td>921.178040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27101.673828</td>\n",
       "      <td>106995.789062</td>\n",
       "      <td>1127.115356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27101.673828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   actualInteractionsPerCrossing  averageInteractionsPerCrossing  \\\n",
       "0                           26.5                            26.5   \n",
       "3                           37.5                            37.5   \n",
       "\n",
       "   correctedActualMu  correctedAverageMu  correctedScaledActualMu  \\\n",
       "0               26.5                26.5                     26.5   \n",
       "3               37.5                37.5                     37.5   \n",
       "\n",
       "   correctedScaledAverageMu  NvtxReco  p_nTracks    p_pt_track     p_eta  ...  \\\n",
       "0                      26.5        18          3  42011.792969  1.834715  ...   \n",
       "3                      37.5        17          2  44975.042969 -2.023659  ...   \n",
       "\n",
       "   p_E5x7_Lr0    p_E5x7_Lr1     p_E5x7_Lr2  p_E5x7_Lr3  p_E7x11_Lr0  \\\n",
       "0  215.588013  43402.332031   74045.820312  337.980713   273.708801   \n",
       "3    0.000000  27372.955078  104002.000000  921.178040     0.000000   \n",
       "\n",
       "    p_E7x11_Lr1    p_E7x11_Lr2  p_E7x11_Lr3  p_E7x7_Lr0    p_E7x7_Lr1  \n",
       "0  43091.683594   74447.539062   470.177124  273.708801  43091.683594  \n",
       "3  27101.673828  106995.789062  1127.115356    0.000000  27101.673828  \n",
       "\n",
       "[2 rows x 160 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /opt/conda/envs/python3/lib/python3.7/site-packages (0.10.0)\n",
      "Collecting update\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/c4/dfe8a392edd35cc635c35cd3b20df6a746aacdeb39b685d1668b56bf819b/update-0.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/envs/python3/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /opt/conda/envs/python3/lib/python3.7/site-packages (from seaborn) (3.1.2)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/envs/python3/lib/python3.7/site-packages (from seaborn) (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/python3/lib/python3.7/site-packages (from seaborn) (1.18.0)\n",
      "Collecting style==1.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/0b/6be2071e20c621e7beb01b86e8474c2ec344a9750ba5315886f24d6e7386/style-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python3/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python3/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python3/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python3/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/python3/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.2->seaborn) (44.0.0.post20200102)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.13.0)\n",
      "Installing collected packages: style, update\n",
      "Successfully installed style-1.1.0 update-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'displot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b1f8c345d4f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'displot'"
     ]
    }
   ],
   "source": [
    "sns.displot(data=train, x=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eventNumber                       0\n",
       "runNumber                         0\n",
       "actualInteractionsPerCrossing     0\n",
       "averageInteractionsPerCrossing    0\n",
       "correctedActualMu                 0\n",
       "                                 ..\n",
       "p_E7x11_Lr2                       0\n",
       "p_E7x11_Lr3                       0\n",
       "p_E7x7_Lr0                        0\n",
       "p_E7x7_Lr1                        0\n",
       "index                             0\n",
       "Length: 166, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nulls\n",
    "### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X = deepcopy(X) \n",
    "# This loop transforms every variable _independently_ \n",
    "for variable in X.columns:     \n",
    "    transformed_X[variable] = RobustScaler().fit_transform(np.array(transformed_X[variable]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select k Best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py:299: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/opt/conda/envs/python3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/opt/conda/envs/python3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/opt/conda/envs/python3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(121495, 15)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_15 = SelectKBest(score_func=f_regression, k=15).fit_transform(transformed_X, y)\n",
    "X_15.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_15,y,test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE is 36932.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# \"Learn\" the mean from the training data\n",
    "mean_train = np.mean(y_train)\n",
    "# Get predictions on the test set\n",
    "baseline_predictions = np.ones(y_test.shape) * mean_train\n",
    "# Compute MAE\n",
    "mae_baseline = mean_absolute_error(y_test, baseline_predictions)\n",
    "print(\"Baseline MAE is {:.2f}\".format(mae_baseline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # xgboost for regression\n",
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# # evaluate the model\n",
    "# model = XGBRegressor(objective='reg:squarederror')\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# n_scores = cross_val_score(model, X_15, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "# # fit the model on the whole dataset\n",
    "# model = XGBRegressor(objective='reg:squarederror')\n",
    "# model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# # evaluate the model\n",
    "# model = LGBMRegressor()\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# n_scores = cross_val_score(model, X_15, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "# # fit the model on the whole dataset\n",
    "# model = LGBMRegressor()\n",
    "# model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 500 candidates, totalling 15000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   34.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 22.6min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 27.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 36.4min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 48.6min\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed: 55.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: -6462.42947586077\n",
      "Best Hyperparameters: {'alpha': 0.0002531181460133387, 'fit_intercept': True, 'normalize': True, 'solver': 'svd'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 15000 out of 15000 | elapsed: 57.1min finished\n"
     ]
    }
   ],
   "source": [
    "# from scipy.stats import loguniform\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# # define model\n",
    "# model = Ridge()\n",
    "# # define evaluation\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # define search space\n",
    "# space = dict()\n",
    "# space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n",
    "# space['alpha'] = loguniform(1e-5, 100)\n",
    "# space['fit_intercept'] = [True, False]\n",
    "# space['normalize'] = [True, False]\n",
    "# # define search\n",
    "# search = RandomizedSearchCV(model, space, n_iter=500, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv, random_state=1, verbose=1)\n",
    "# # execute search\n",
    "# result = search.fit(X_15, y)\n",
    "# # summarize result\n",
    "# print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 128 candidates, totalling 3840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3840 out of 3840 | elapsed: 14.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: -6464.314837511495\n",
      "Best Hyperparameters: {'alpha': 0.0001, 'fit_intercept': True, 'normalize': True, 'solver': 'sag'}\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # define model\n",
    "# model = Ridge()\n",
    "# # define evaluation\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # define search space\n",
    "# space = dict()\n",
    "# space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n",
    "# space['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "# space['fit_intercept'] = [True, False]\n",
    "# space['normalize'] = [True, False]\n",
    "# # define search\n",
    "# search = GridSearchCV(model, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv, verbose=1)\n",
    "# # execute search\n",
    "# result = search.fit(X_15, y)\n",
    "# # summarize result\n",
    "# print('Best Score: %s' % result.best_score_)\n",
    "# print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 9 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 23.6min\n"
     ]
    }
   ],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # define model\n",
    "# model = XGBRegressor()\n",
    "# # define evaluation\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # define search space\n",
    "# parameters = {\n",
    "#               'objective':['reg:linear'],\n",
    "#               'learning_rate': [.03, 0.05, .07], #so called `eta` value\n",
    "#               'max_depth': [5, 6, 7],\n",
    "#               'min_child_weight': [4],\n",
    "#               'silent': [1],\n",
    "#               'subsample': [0.7],\n",
    "#               'colsample_bytree': [0.7],\n",
    "#               'n_estimators': [500]}\n",
    "# # define search\n",
    "# search = GridSearchCV(model, parameters, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv, verbose=1)\n",
    "# # execute search\n",
    "# result = search.fit(X_15, y)\n",
    "# # summarize result\n",
    "# print('Best Score: %s' % result.best_score_)\n",
    "# print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimisation of hyperparameters and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklNN_CrossValidation(hidden_layer_sizes, learning_rate_init, data, targets):\n",
    "    \"\"\"Cross validation.\n",
    "       Fits a NN with the given paramaters to the target \n",
    "       given data, calculated a CV accuracy score and returns the mean.\n",
    "       The goal is to find combinations\n",
    "       that maximize the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    estimator = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=learning_rate_init, random_state=0)\n",
    "    \n",
    "    cval = cross_val_score(estimator, data, targets, scoring='neg_mean_absolute_error', cv=5)\n",
    "    \n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_sklNN(data, targets, pars, n_iter=5):\n",
    "    \"\"\"Apply Bayesian Optimization to NN parameters.\"\"\"\n",
    "    \n",
    "    def crossval_wrapper(hidden_layer_sizes, learning_rate_init):\n",
    "        \"\"\"Wrapper of NNe cross validation. \n",
    "           hidden_layer_sizes\n",
    "           is cast to integer before we pass them along.\n",
    "        \"\"\"\n",
    "        return sklNN_CrossValidation(hidden_layer_sizes=int(hidden_layer_sizes), \n",
    "                                            learning_rate_init=learning_rate_init, \n",
    "                                            data=data, \n",
    "                                            targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(f=crossval_wrapper, \n",
    "                                     pbounds=pars, \n",
    "                                     random_state=42, \n",
    "                                     verbose=100)\n",
    "    optimizer.maximize(init_points=4, n_iter=n_iter)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | hidden... | learni... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-6.947e+0\u001b[0m | \u001b[0m 38.08   \u001b[0m | \u001b[0m 0.9507  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-6.409e+0\u001b[0m | \u001b[95m 73.47   \u001b[0m | \u001b[95m 0.5987  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-6.515e+0\u001b[0m | \u001b[0m 16.45   \u001b[0m | \u001b[0m 0.1561  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-6.461e+0\u001b[0m | \u001b[0m 6.75    \u001b[0m | \u001b[0m 0.8662  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-6.531e+0\u001b[0m | \u001b[0m 73.55   \u001b[0m | \u001b[0m 0.5979  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-6.531e+0\u001b[0m | \u001b[0m 73.47   \u001b[0m | \u001b[0m 0.598   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-6.46e+03\u001b[0m | \u001b[0m 6.747   \u001b[0m | \u001b[0m 0.8607  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-6.462e+0\u001b[0m | \u001b[0m 30.77   \u001b[0m | \u001b[0m 0.4783  \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m-6.388e+0\u001b[0m | \u001b[95m 73.47   \u001b[0m | \u001b[95m 0.5976  \u001b[0m |\n",
      "=================================================\n",
      "{'target': -6387.571888318518, 'params': {'hidden_layer_sizes': 73.46755675774493, 'learning_rate_init': 0.5975716395036551}}\n"
     ]
    }
   ],
   "source": [
    "parameters_BayesianOptimization = {\"hidden_layer_sizes\": (1, 100), \n",
    "                                   \"learning_rate_init\": (0.0001, 1)\n",
    "                                  }\n",
    "\n",
    "BayesianOptimization = optimize_sklNN(X_15, \n",
    "                                             y, \n",
    "                                             parameters_BayesianOptimization, \n",
    "                                             n_iter=5)\n",
    "print(BayesianOptimization.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation on result:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6387.5719 MAE with a standard deviation of 121.9844\n"
     ]
    }
   ],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=int(BayesianOptimization.max['params']['hidden_layer_sizes']), \n",
    "                                 learning_rate_init=BayesianOptimization.max['params']['learning_rate_init'],\n",
    "                                 random_state=0)\n",
    "scores = cross_val_score(reg, X_15, y, cv=5, scoring='neg_mean_absolute_error') \n",
    "print(f\"{scores.mean():.4f} MAE with a standard deviation of {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BayesianOptimization' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-0c660e4ac030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                              \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                              \u001b[0mparameters_BayesianOptimization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                              n_iter=5)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-61b1dd2caead>\u001b[0m in \u001b[0;36moptimize_sklNN\u001b[0;34m(data, targets, pars, n_iter)\u001b[0m\n\u001b[1;32m     15\u001b[0m                                      \u001b[0mpbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                      \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                      verbose=100)\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BayesianOptimization' object is not callable"
     ]
    }
   ],
   "source": [
    "parameters_BayesianOptimization = {\"hidden_layer_sizes\": (60, 90), \n",
    "                                   \"learning_rate_init\": (0.3, 0.6)\n",
    "                                  }\n",
    "\n",
    "BayesianOptimization = optimize_sklNN(X_15, \n",
    "                                             y, \n",
    "                                             parameters_BayesianOptimization, \n",
    "                                             n_iter=5)\n",
    "print(BayesianOptimization.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
