{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red0\green0\blue0;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0\c87059;\cssrgb\c100000\c100000\c100000\c0;\cssrgb\c0\c0\c0;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww15340\viewh9240\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 1. Classification_NiallGray_LightGBMAlgo.txt\
Algorithm: LightGBMClassifier \
HP values: \cf2 \cb3 \expnd0\expndtw0\kerning0
lambda_l1: 1.619246801524985e-08,  lambda_l2: 9.781097821392834, num_leaves: 31, feature_fraction: 0.748,\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3     bagging_fraction: 1.0, bagging_freq: 0, min_child_samples: 20\
HP optimisation: Used Optuna tuner integration with LightGBM with CV.\
Accuracy mean and std from 10-fold CV: 0.943 (0.002) \
Feature selection: SHAP values found for whole set and top 25 used for final training\
Evaluation: Seems to perform well.\
\
2.\cf0 \cb1 \kerning1\expnd0\expndtw0  Classification_NiallGray_xgboost\
Algorithm: XGBoostClassifier\
HP values: learning_rate: 0.05818, max_depth: 4, n_estimators: 320.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \expnd0\expndtw0\kerning0
HP optimisation\cf4 : Bayesian optimisation \
\cf2 Feature selection: Used 25 found by SHAP previously\
Evaluation: Seems OK but much slower than LightGBM algo.\
\
3. Classification_NiallGray_MLPalgoBayesOpt.txt\
Algorithm: SKLearn MLPClassifier\
HP values: 'hidden_layer_sizes': 100.0, 'learning_rate_init': 0.0001\
HP optimisation\cf4 : Bayesian optimisation \
\cf2 Accuracy with CV: 0.9204 accuracy with a standard deviation of 0.0007\
Evaluation: Decent but the accuracy isn\'92t as good as the tree methods I performed.\
\
4. Regression_NiallGray_ridge.txt\
Algorithm: SKLearn Ridge regression\
HP values: 'alpha': 10, 'fit_intercept': True, 'normalize': False, 'solver': 'cholesky'\
HP optimisation: GridSearchCV\
Performance: MAE = -6462.667275597323\
Feature selection: SelectKBest with f_regression\
Evaluation: Fast, and MAE better than my XGBoost and LightGBM attempts so worth including\
\
5. Regression_NiallGray_MLPReg.txt\
Algorithm: SKLearn MLPRegressor\
HP values: 'hidden_layer_sizes': 71.30506683048498, 'learning_rate_init': 0.5687396572526524\}\
HP optimisation\cf4 : Bayesian optimisation \
\cf2 Performance: -6366.1829 MAE with a standard deviation of 87.2863\
Evaluation: Seems decent - difficult to gauge what the MAE means relatively\
\
6. Regression_NiallGray_AutoKeras.txt\
Algorithm: AutoKeras StructuredDataRegressor\
HP optimisation: Neural architecture search - automatic HP and model architecture searching\
Performance: MAE: 6133.658\
Evaluation: Took about 8 hours to train but requires minimal manual tuning and found the best MAE of all my regressors\
\
7. Clustering_NiallGray_GaussMix10feat.txt\
Algorithm: SKLearn GaussianMixture\
Dimensionality reduction: UMAP reduced to 4 components with n_neighbors=30, min_dist=0.0\
Features: Best 10\
Evaluation: Decent clustering in plot, and distribution looks sensible\
\
8. Clustering_NiallGray_GaussMix6feat.txt\
Algorithm: SKLearn GaussianMixture\
Dimensionality reduction: UMAP reduced to 4 components with n_neighbors=30, min_dist=0.0\
Features: Best 6\
Evaluation: Decent clustering in plot (three regions), and distribution looks sensible\
\
9. Clustering_NiallGray_KMeans6feat.txt\
Algorithm: K Means clustering\
Params: , init='k-means++', n_init=10,\
Dimensionality reduction: UMAP reduced to 4 components with n_neighbors=30, min_dist=0.0\
Features: Best 6\
Evaluation: Clustering doesn\'92t look as nice, but still decent distribution of groups.\
\
\
\
\
\
\
}